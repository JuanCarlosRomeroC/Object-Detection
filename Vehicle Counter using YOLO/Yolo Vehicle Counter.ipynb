{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":26,"outputs":[{"output_type":"stream","text":"/kaggle/input/yolo-coco-data/yolov3.cfg\n/kaggle/input/yolo-coco-data/coco.names\n/kaggle/input/yolo-coco-data/yolov3.weights\n/kaggle/input/rahulkvideo/new.mp4\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2 \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The class names are there in coco.names files.\n# The pretrained weights of YOLO are there in yolov3.weights.\n# And yolov3.config is the configuration file.."},{"metadata":{"trusted":true},"cell_type":"code","source":"names=open(\"/kaggle/input/yolo-coco-data/coco.names\").read()\nnames","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'person\\nbicycle\\ncar\\nmotorbike\\naeroplane\\nbus\\ntrain\\ntruck\\nboat\\ntraffic light\\nfire hydrant\\nstop sign\\nparking meter\\nbench\\nbird\\ncat\\ndog\\nhorse\\nsheep\\ncow\\nelephant\\nbear\\nzebra\\ngiraffe\\nbackpack\\numbrella\\nhandbag\\ntie\\nsuitcase\\nfrisbee\\nskis\\nsnowboard\\nsports ball\\nkite\\nbaseball bat\\nbaseball glove\\nskateboard\\nsurfboard\\ntennis racket\\nbottle\\nwine glass\\ncup\\nfork\\nknife\\nspoon\\nbowl\\nbanana\\napple\\nsandwich\\norange\\nbroccoli\\ncarrot\\nhot dog\\npizza\\ndonut\\ncake\\nchair\\nsofa\\npottedplant\\nbed\\ndiningtable\\ntoilet\\ntvmonitor\\nlaptop\\nmouse\\nremote\\nkeyboard\\ncell phone\\nmicrowave\\noven\\ntoaster\\nsink\\nrefrigerator\\nbook\\nclock\\nvase\\nscissors\\nteddy bear\\nhair drier\\ntoothbrush'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"names=names.strip().split(\"\\n\")","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(names)","execution_count":30,"outputs":[{"output_type":"stream","text":"['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(names))","execution_count":31,"outputs":[{"output_type":"stream","text":"80\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Now we have got the labels and we saw that the Coco dataset consists of 80 classes.."},{"metadata":{},"cell_type":"markdown","source":"# Now defining paths to the weights and configuration file:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_path = '/kaggle/input/yolo-coco-data/yolov3.weights'\nconfiguration_path = '/kaggle/input/yolo-coco-data/yolov3.cfg'\n\npro_min = 0.5 # Setting minimum probability to eliminate weak predictions\n\nthreshold = 0.3 # Setting threshold for non maximum suppression","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now loading YOLO Object Detector with the help of DNN(Deep Neural Network) Library:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"net = cv2.dnn.readNetFromDarknet(configuration_path,weights_path)\n\n# Getting names of all layers\nlayers = net.getLayerNames()  # list of layers' names\n\n# # Check point\nprint(layers)","execution_count":33,"outputs":[{"output_type":"stream","text":"['conv_0', 'bn_0', 'relu_1', 'conv_1', 'bn_1', 'relu_2', 'conv_2', 'bn_2', 'relu_3', 'conv_3', 'bn_3', 'relu_4', 'shortcut_4', 'conv_5', 'bn_5', 'relu_6', 'conv_6', 'bn_6', 'relu_7', 'conv_7', 'bn_7', 'relu_8', 'shortcut_8', 'conv_9', 'bn_9', 'relu_10', 'conv_10', 'bn_10', 'relu_11', 'shortcut_11', 'conv_12', 'bn_12', 'relu_13', 'conv_13', 'bn_13', 'relu_14', 'conv_14', 'bn_14', 'relu_15', 'shortcut_15', 'conv_16', 'bn_16', 'relu_17', 'conv_17', 'bn_17', 'relu_18', 'shortcut_18', 'conv_19', 'bn_19', 'relu_20', 'conv_20', 'bn_20', 'relu_21', 'shortcut_21', 'conv_22', 'bn_22', 'relu_23', 'conv_23', 'bn_23', 'relu_24', 'shortcut_24', 'conv_25', 'bn_25', 'relu_26', 'conv_26', 'bn_26', 'relu_27', 'shortcut_27', 'conv_28', 'bn_28', 'relu_29', 'conv_29', 'bn_29', 'relu_30', 'shortcut_30', 'conv_31', 'bn_31', 'relu_32', 'conv_32', 'bn_32', 'relu_33', 'shortcut_33', 'conv_34', 'bn_34', 'relu_35', 'conv_35', 'bn_35', 'relu_36', 'shortcut_36', 'conv_37', 'bn_37', 'relu_38', 'conv_38', 'bn_38', 'relu_39', 'conv_39', 'bn_39', 'relu_40', 'shortcut_40', 'conv_41', 'bn_41', 'relu_42', 'conv_42', 'bn_42', 'relu_43', 'shortcut_43', 'conv_44', 'bn_44', 'relu_45', 'conv_45', 'bn_45', 'relu_46', 'shortcut_46', 'conv_47', 'bn_47', 'relu_48', 'conv_48', 'bn_48', 'relu_49', 'shortcut_49', 'conv_50', 'bn_50', 'relu_51', 'conv_51', 'bn_51', 'relu_52', 'shortcut_52', 'conv_53', 'bn_53', 'relu_54', 'conv_54', 'bn_54', 'relu_55', 'shortcut_55', 'conv_56', 'bn_56', 'relu_57', 'conv_57', 'bn_57', 'relu_58', 'shortcut_58', 'conv_59', 'bn_59', 'relu_60', 'conv_60', 'bn_60', 'relu_61', 'shortcut_61', 'conv_62', 'bn_62', 'relu_63', 'conv_63', 'bn_63', 'relu_64', 'conv_64', 'bn_64', 'relu_65', 'shortcut_65', 'conv_66', 'bn_66', 'relu_67', 'conv_67', 'bn_67', 'relu_68', 'shortcut_68', 'conv_69', 'bn_69', 'relu_70', 'conv_70', 'bn_70', 'relu_71', 'shortcut_71', 'conv_72', 'bn_72', 'relu_73', 'conv_73', 'bn_73', 'relu_74', 'shortcut_74', 'conv_75', 'bn_75', 'relu_76', 'conv_76', 'bn_76', 'relu_77', 'conv_77', 'bn_77', 'relu_78', 'conv_78', 'bn_78', 'relu_79', 'conv_79', 'bn_79', 'relu_80', 'conv_80', 'bn_80', 'relu_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'relu_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'relu_88', 'conv_88', 'bn_88', 'relu_89', 'conv_89', 'bn_89', 'relu_90', 'conv_90', 'bn_90', 'relu_91', 'conv_91', 'bn_91', 'relu_92', 'conv_92', 'bn_92', 'relu_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'relu_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'relu_100', 'conv_100', 'bn_100', 'relu_101', 'conv_101', 'bn_101', 'relu_102', 'conv_102', 'bn_102', 'relu_103', 'conv_103', 'bn_103', 'relu_104', 'conv_104', 'bn_104', 'relu_105', 'conv_105', 'permute_106', 'yolo_106']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Getting the output layers:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in net.getUnconnectedOutLayers():\n    print(layers[i[0]-1])","execution_count":34,"outputs":[{"output_type":"stream","text":"yolo_82\nyolo_94\nyolo_106\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Generally in a sequential CNN network there will be only one output layer at the end. In the YOLO v3 architecture we are using there are multiple output layers giving out predictions. get_output_layers() function gives the names of the output layers. An output layer is not connected to any next layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"output_layers=[layers[i[0] - 1] for i in net.getUnconnectedOutLayers()] # We are searching for unconnected layers as output layers are not connected with any layer.\n\nprint(output_layers)","execution_count":35,"outputs":[{"output_type":"stream","text":"['yolo_82', 'yolo_94', 'yolo_106']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# So these are the output layers.."},{"metadata":{},"cell_type":"markdown","source":"# For Different colours of different Objects:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"colours = np.random.randint(0, 255, size=(len(names), 3), dtype='uint8') # randint(low, high=None, size=None, dtype='l')\n\nprint(colours.shape)\nprint(len(colours))\nprint(colours[0])  ","execution_count":36,"outputs":[{"output_type":"stream","text":"(80, 3)\n80\n[138 209 187]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Made a list of colours with randomly RGB colour of each class.\nMaking so that we can distinguish every objects with different colours.."},{"metadata":{},"cell_type":"markdown","source":"# Reading the Video for Counting Of Objects:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"video=cv2.VideoCapture(\"/kaggle/input/rahulkvideo/new.mp4\")\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for out in output:\n#     for res in out:\n#         print(len(res[5:]))  \n#     print('************************')","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for out in output:\n#      for res in out:\n        \n#         # print(res)\n#         # Getting class for current object\n#         scores = res[5:]\n#         print(res[0])","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are taking res[5:] because we have 80 classes which are there after the first 5 values as the bounding box dimensions. So Leaving the first 5 values and searching the class in the next 80 classes.\n# Then getting class for current object by argmax function. Argmax function returns the indices of the maximum value in the array.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"classname = []\nlist_of_vehicles = [\"bicycle\",\"car\",\"motorbike\",\"bus\",\"truck\"]","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vehicle_count(boxes, class_names):\n\ttotal_vehicle_count = 0 # total vechiles present in the image\n\tdict_vehicle_count = {} # dictionary with count of each distinct vehicles detected\n\tfor i in range(len(boxes)):\n\t\tclass_name = class_names[i]\n\t\t# print(i,\".\",class_name)\n\t\tif(class_name in list_of_vehicles):\n\t\t\ttotal_vehicle_count += 1\n\t\t\tdict_vehicle_count[class_name] = dict_vehicle_count.get(class_name,0) + 1\n\n\treturn total_vehicle_count, dict_vehicle_count","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n\tprop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \\\n\t\telse cv2.CAP_PROP_FRAME_COUNT\n\ttotal = 200\n\tprint(\"[INFO] {} total frames in video\".format(total))\n\n# an error occurred while trying to determine the total\n# number of frames in the video file\nexcept:\n\tprint(\"[INFO] could not determine # of frames in video\")\n\tprint(\"[INFO] no approx. completion time can be provided\")\n\ttotal = -1","execution_count":42,"outputs":[{"output_type":"stream","text":"[INFO] could not determine # of frames in video\n[INFO] no approx. completion time can be provided\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Creating frame in a loop\nwhile True:\n\t# read the next frame from the file\n\t(grabbed, frame) = video.read()\n\n\t# if the frame was not grabbed, then we have reached the end\n\t# of the stream\n\tif not grabbed:\n\t\tbreak\n\n\t# if the frame dimensions are empty, grab them\n\tif w is None or h is None:\n\t\t(h,w) = frame.shape[:2]\n\n\t# construct a blob from the input frame and then perform a forward\n\t# pass of the YOLO object detector, giving us our bounding boxes\n\t# and associated probabilities\n\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n\t\tswapRB=True, crop=False)\n\tnet.setInput(blob)\n\tstart = time.time()\n\tlayerOutputs = net.forward(output_layers)\n\tend = time.time()\n\n\t# initialize our lists of detected bounding boxes, confidences,\n\t# and class IDs, respectively\n\tboxes = []\n\tconfidences = []\n\tclasses = []\n\n\t# loop over each of the layer outputs\n\tfor output in layerOutputs:\n\t\t# loop over each of the detections\n\t\tfor detection in output:\n\t\t\t# extract the class ID and confidence (i.e., probability)\n\t\t\t# of the current object detection\n\t\t\tscores = detection[5:]\n            \n\t\t\tclasss = np.argmax(scores)\n            \n\t\t\tconfidence = scores[classs]\n\n\t\t\t# filter out weak predictions by ensuring the detected\n\t\t\t# probability is greater than the minimum probability\n\t\t\tif confidence > 0.5:\n\t\t\t\t# scale the bounding box coordinates back relative to\n\t\t\t\t# the size of the image, keeping in mind that YOLO\n\t\t\t\t# actually returns the center (x, y)-coordinates of\n\t\t\t\t# the bounding box followed by the boxes' width and\n\t\t\t\t# height\n\t\t\t\tbox = detection[0:4] * np.array([w,h,w,h])\n\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n\n\t\t\t\t# use the center (x, y)-coordinates to derive the top\n\t\t\t\t# and and left corner of the bounding box\n\t\t\t\tx = int(centerX - (width / 2))\n\t\t\t\ty = int(centerY - (height / 2))\n\n\t\t\t\t# update our list of bounding box coordinates,\n\t\t\t\t# confidences, and class IDs\n\t\t\t\tboxes.append([x, y, int(width), int(height)])\n\t\t\t\tconfidences.append(float(confidence))\n\t\t\t\tclasses.append(classs)\n\t\t\t\tclassname.append(names[classs])\n\n\t# apply non-maxima suppression to suppress weak, overlapping\n\t# bounding boxes\n\tidxs = cv2.dnn.NMSBoxes(boxes, confidences,0.5,\n\t\t0.4)\n\n\t# ensure at least one detection exists\n\tif len(idxs) > 0:\n\t\t# loop over the indexes we are keeping\n\t\tfor i in idxs.flatten():\n\t\t\t# extract the bounding box coordinates\n\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n\n\t\t\t# draw a bounding box rectangle and label on the frame\n\t\t\tcolor = [int(c) for c in colours[classes[i]]]\n\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n\t\t\ttext = \"{}: {:.4f}\".format(names[classes[i]],\n\t\t\t\tconfidences[i])\n\t\t\tcv2.putText(frame, text, (x, y - 5),\n\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n\t# check if the video writer is None\n\n\ttotal_vehicles, each_vehicle = get_vehicle_count(boxes, classname)\n\tprint(\"Total vehicles in the Image\", total_vehicles)\n\tprint(\"Each Vehicle Count in the image\", each_vehicle)\n\tif writer is None:\n\t\t# initialize our video writer\n\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n\t\twriter = cv2.VideoWriter('video_final.mp4', fourcc, 30,\n\t\t\t(frame.shape[1], frame.shape[0]), True)\n\n\t\t# some information on processing single frame\n\t\tif total > 0:\n\t\t\telap = (end - start)\n\t\t\tprint(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n\t\t\tprint(\"[INFO] estimated total time to finish: {:.4f}\".format(\n\t\t\t\telap * total))\n\n\t# write the output frame to disk\n\twriter.write(frame)\n\n# release the file pointers\nprint(\"[INFO] cleaning up...\")\nwriter.release()\nvideo.release()","execution_count":43,"outputs":[{"output_type":"stream","text":"Total vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 23\nEach Vehicle Count in the image {'car': 23}\nTotal vehicles in the Image 24\nEach Vehicle Count in the image {'car': 24}\nTotal vehicles in the Image 22\nEach Vehicle Count in the image {'car': 22}\nTotal vehicles in the Image 24\nEach Vehicle Count in the image {'car': 24}\nTotal vehicles in the Image 23\nEach Vehicle Count in the image {'car': 23}\nTotal vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 24\nEach Vehicle Count in the image {'car': 24}\nTotal vehicles in the Image 23\nEach Vehicle Count in the image {'car': 23}\nTotal vehicles in the Image 23\nEach Vehicle Count in the image {'car': 23}\nTotal vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 17\nEach Vehicle Count in the image {'car': 17}\nTotal vehicles in the Image 16\nEach Vehicle Count in the image {'car': 16}\nTotal vehicles in the Image 17\nEach Vehicle Count in the image {'car': 17}\nTotal vehicles in the Image 17\nEach Vehicle Count in the image {'car': 17}\nTotal vehicles in the Image 18\nEach Vehicle Count in the image {'car': 18}\nTotal vehicles in the Image 18\nEach Vehicle Count in the image {'car': 18}\nTotal vehicles in the Image 18\nEach Vehicle Count in the image {'car': 18}\nTotal vehicles in the Image 18\nEach Vehicle Count in the image {'car': 18}\nTotal vehicles in the Image 18\nEach Vehicle Count in the image {'car': 18}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 20\nEach Vehicle Count in the image {'car': 20}\nTotal vehicles in the Image 20\nEach Vehicle Count in the image {'car': 20}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\nTotal vehicles in the Image 20\nEach Vehicle Count in the image {'car': 20}\nTotal vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 20\nEach Vehicle Count in the image {'car': 20}\nTotal vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 21\nEach Vehicle Count in the image {'car': 21}\nTotal vehicles in the Image 20\nEach Vehicle Count in the image {'car': 20}\nTotal vehicles in the Image 19\nEach Vehicle Count in the image {'car': 19}\n[INFO] cleaning up...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink('video_final.mp4')","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"/kaggle/working/video_final.mp4","text/html":"<a href='video_final.mp4' target='_blank'>video_final.mp4</a><br>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# So we are done with YOLO.."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}